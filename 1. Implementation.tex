\setcounter{secnumdepth}{4}

\clearpage

\section{Implementation}
\label{sec:implementation}


\subsection{Hard-coded solution}
To get familiar with the IR-Sim library, we first hard-coded a robot to move in a circle. To do this we first found the source code for the \textit{Dash} behaviour when using \textit{Diff} kinematics\parencite{website:DiffDash}. This basic behaviour simply makes the agent move directly towards the goal, slowing down to turn if necessary. It does so by calculating the difference in the angle of the agent and the angle between the agent and the goal, which is the angle the agent should have to move towards the goal, and the agent then simply turns to reduce this difference to zero.

To make the agent instead move along a circle, we simply add 90 degrees to the angle between the agent and the goal. This makes the agent move along the circle tangent and assuming infinetly small steps, this would result in a perfect circle. However since we don't have infinetly small steps, the agent will move further and further away from the circle with each step. To account for this, we simply take the distance from the circle center and add this to the equation, increasing the angle when the distance is less than the desired radius and decreasing the angle when the distance is more than the desired radius. This results in the code seen in listing \ref{code:90Degrees}.

\begin{codeblock}{python}{Implementation of getting the desired angle for the agent}{code:90Degrees}
# Get the distance and the angle to the goal (Circle center):
distance, radian = relative_position(state, goal)

# Calculate the distance correction amount:
distanceCorrection = (circle_radius - distance) * correction_multiplier

# Add 90 degress + the distance correction to the angle between the agent and the goal
radian += np.pi/2 + distanceCorrection
\end{codeblock}

\subsection{RL-learning}


\subsubsection{Alba}


\subsubsection{Q learning with outside table and error shaped reward}
This solution uses predefined actions and states to fill out an outside Q-table by learning throughout multiple episodes. The predefined states are determined by using data binning, meaning we sort the current state into categories. These categories are determined by computing a relative distance and angle from the robot's pose $(x,y,\theta)$. From these we get a radial and an angle error value, from which we can discretize a $3\times3$ state matrix using the bins:
\begin{itemize}
    \item Radius bins: $(-\infty,-0.2]$, $[-0.2, 0.2]$, $[0.2,\infty)$ \; (inside / on / outside).
    \item Angle bins: $(-\pi,-\pi/4]$, $[-\pi/4, \pi/4]$, $[\pi/4,\pi)$ \; (left / aligned / right).
\end{itemize}
The robot's actions are defined by 4 simple actions. These were originally meant to be used for test purposes, but the final results were accurate enough that we didn't need to implement more actions. The actions work by changing the linear and angular velocities of the robot $(v,\omega)$. The actions are built from the robot's limits $(v_{\max}, \omega_{\max})$: fast means $80\%$ of the max linear speed, slow means $40\%$, and turning actions use $80\%$ of the max angular speed:
\begin{itemize}
    \item fast forward + turn left
    \item fast forward straight
    \item fast forward + turn right
    \item slow forward straight
\end{itemize}
The learning is a Q-learning algorithm, where the learning rate is $0.5$, the discount is $0.9$, and the $\varepsilon$-greedy rate is $0.2$. After each transition the update is:
\[
Q^\Pi (s,a) \leftarrow Q^\Pi (s,a) + \alpha \bigl( R + \gamma max_{a} Q^\Pi(s',a') - Q^\Pi(s,a) \bigr).
\]
The reward is shaped by penalising radial and heading errors, while rewarding progress along the orbit. Weights are added for normalisation, as you can see in \ref{code:shapedReward}:

\begin{codeblock}{python}{Shaped reward function}{code:shapedReward}
def shaped_reward(e_r: float, e_psi: float, v_cmd: float) -> float:
    alpha = 2.0    # weight radial error
    beta  = 1.0    # weight heading error
    tan   = 0.4    # weight tangential
    return float(-alpha * abs(e_r) - beta * abs(e_psi) + tan * (v_cmd * math.cos(e_psi)))
\end{codeblock}

We used a CSV for the Q table so the learning progress can be saved throughout iterations, and so the model can be trained throughout multiple epochs.


\subsubsection{Wouter Jonathan}


\subsection{Subsumption Architecture and Obstacle Avoidance}
